{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengenalan Data Pipeline menggunakan TensorFlow\n",
    "\n",
    "Import library yang akan digunakan seperti berikut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita bisa membuat data secara langsung di dalam memory seperti berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat data sederhana\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])  # y = 2x\n",
    "\n",
    "# Membuat dataset dari numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "# Melihat isi dataset\n",
    "for item in dataset:\n",
    "    print(f\"x: {item[0].numpy()}, y: {item[1].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beberapa operasi dasar pun bisa kita lakukan secara langsung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat dataset yang lebih besar\n",
    "x_larger = np.arange(20)\n",
    "y_larger = x_larger * 2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_larger, y_larger))\n",
    "\n",
    "# 1. Shuffling (mengacak data)\n",
    "shuffled_dataset = dataset.shuffle(buffer_size=100)\n",
    "\n",
    "# 2. Batching (mengelompokkan data)\n",
    "batched_dataset = shuffled_dataset.batch(batch_size=4)\n",
    "\n",
    "# 3. Repeat dataset\n",
    "repeated_dataset = batched_dataset.repeat(count=2)\n",
    "\n",
    "print(\"Dataset dalam batch:\")\n",
    "for batch in batched_dataset.take(3):\n",
    "    print(f\"Batch x: {batch[0].numpy()}\")\n",
    "    print(f\"Batch y: {batch[1].numpy()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga bisa meng-mapping suatu fungsi agar di-apply ke seluruh datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk normalisasi data\n",
    "def normalize(x, y):\n",
    "    return tf.cast(x, tf.float32) / 20.0, y\n",
    "\n",
    "# Menerapkan normalisasi ke dataset\n",
    "normalized_dataset = dataset.map(normalize)\n",
    "\n",
    "print(\"Data setelah normalisasi:\")\n",
    "for x, y in normalized_dataset.take(5):\n",
    "    print(f\"x: {x.numpy():.3f}, y: {y.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data juga bisa dibuat dalam bentuk CSV, namun method ini merupakan bagian dari experimental TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pertama, buat file CSV sederhana untuk contoh\n",
    "import csv\n",
    "\n",
    "# Membuat data contoh\n",
    "with open('sample_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['x', 'y'])\n",
    "    for i in range(10):\n",
    "        writer.writerow([i, i*2])\n",
    "\n",
    "# Membuat dataset dari file CSV\n",
    "csv_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    'sample_data.csv',\n",
    "    batch_size=3,\n",
    "    label_name='y',\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "print(\"Data dari CSV:\")\n",
    "for features, labels in csv_dataset.take(3):\n",
    "    print(f\"Features: {features['x'].numpy()}\")\n",
    "    print(f\"Labels: {labels.numpy()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga bisa melakukan beberapa teknik optimasi dengan melakukan data preproses dan training secara bersamaan. Teknik ini dinamakan prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat pipeline lengkap dengan prefetch\n",
    "BATCH_SIZE = 4\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "final_dataset = tf.data.Dataset.from_tensor_slices((x_larger, y_larger))\n",
    "final_dataset = final_dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "final_dataset = final_dataset.batch(BATCH_SIZE)\n",
    "final_dataset = final_dataset.map(normalize)\n",
    "final_dataset = final_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Pipeline lengkap dengan prefetch:\")\n",
    "for x, y in final_dataset.take(2):\n",
    "    print(f\"Batch x: {x.numpy()}\")\n",
    "    print(f\"Batch y: {y.numpy()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lalu selanjutnya data dapat dimasukkan ke dalam model untuk training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catatan Penting:\n",
    "\n",
    "1. `shuffle()`: Mengacak data untuk training yang lebih baik\n",
    "   - `buffer_size` menentukan seberapa acak datanya\n",
    "   \n",
    "2. `batch()`: Mengelompokkan data menjadi batch\n",
    "   - Penting untuk training yang efisien\n",
    "   - Ukuran batch memengaruhi memori dan kecepatan training\n",
    "   \n",
    "3. `prefetch()`: Mempersiapkan data selanjutnya saat CPU/GPU sedang memproses\n",
    "   - Menggunakan `tf.data.AUTOTUNE` untuk optimasi otomatis\n",
    "   \n",
    "4. `map()`: Mentransformasi data\n",
    "   - Bisa digunakan untuk preprocessing, augmentasi, dll\n",
    "   - Dapat diparalelkan dengan `num_parallel_calls`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
